---
layout: default
title: Ke Lin
---


<h2>About Me</h2>

<div class="aboutme">
  <div class="aboutme-content">
    <p>
      I am a second-year master student in Software Engineering, Tsinghua University, supervised by <a
        href="https://www.thss.tsinghua.edu.cn/en/faculty/pingluo.htm">Prof. Ping Luo</a>. I received my
      bachelor's
      degree in Software Engineering from Tsinghua University, advised by <a
        href="https://www.thss.tsinghua.edu.cn/en/faculty/lijiewen.htm">Prof. Lijie Wen</a>.
    </p>
    <p>I am currently a research intern at <a href="https://gcloud.tencent.com/pages/research/game-ai.html">Tencent
        Game AI Research Center</a>, with a mission to improve
      the realistic, reliability, and aesthetic of AI-generated 3D scenes.</p>
    <p style="color: rgb(205,97,85)">ðŸŒŸSeeking open positions for 2025 Fall Ph.D.ðŸŒŸ</p>
  </div>
  <div><img class="profile-picture" src="img/profile.jpg" /></div>
</div>

<hr />

<h2>Education</h2>

<div class="education">
  <div class="education-block">
    <img src="img/tsinghua-university-logo.png" />
    <div>
      <div class="education-school">Tsinghua University</div>
      <div class="education-date">2022.09 - 2025.06 (expected)</div>
      <div class="education-degree">Master of Engineering in Software Engineering</div>
      <div class="education-score">GPA: 3.52/4.0</div>
    </div>
  </div>
  <div class="education-block">
    <img src="img/tsinghua-university-logo.png" />
    <div>
      <div class="education-school">Tsinghua University</div>
      <div class="education-date">2018.09 - 2022.06</div>
      <div class="education-degree">Bachelor in Software Engineering</div>
      <div class="education-score">GPA: 3.69/4.0, Rank 25%</div>
    </div>
  </div>
</div>

<hr />

<h2>Research of Interest</h2>

<p>Previously, I focused on AI safety, including LLM Watermark, Linguistic Steganography and Privacy-preserving
  Computation.</p>

<p>My current Research of Interest are:</p>
<ul>
  <li>3D Scene Layout</li>
  <li>LLM-guided 3D Vision</li>
  <li>Multi-modal 3D LLMs</li>
  <li>Compositional 3D Vision</li>
</ul>

<hr />

<h2>Research Experiences</h2>

<div class="research">
  <div class="research-block">
    <img src="img/tencent-logo.png" />
    <div>
      <div class="research-institute"><a href="https://gcloud.tencent.com/pages/research/game-ai.html">Tencent -
          Game AI Research Center</a></div>
      <div class="research-date">2024.04 - Present</div>
      <div class="research-role">Research Intern</div>
    </div>
  </div>
  <div class="research-block">
    <img src="img/momenta-logo.png" />
    <div>
      <div class="research-institute"><a href="https://www.momenta.cn/en/">Momenta</a></div>
      <div class="research-date">2021.01 - 2021.04</div>
      <div class="research-role">Research & Development Intern</div>
    </div>
  </div>
</div>

<hr />

<h2>Selected Publications</h2>

<div class="publication">
  <p>* denotes equal contribution.</p>
  <div class="pub-block">
    <img src="img/paper/calpcg.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="https://arxiv.org/abs/2311.16501">
              Context-Aware Indoor Point Cloud Object Generation through User Instructions
            </a></div>
          <div class="pub-author">Yiyang Luo*, <b>Ke Lin*</b>, Chao Gu</div>
          <div class="pub-conf">ACM MM 2024</div>
          <div>
            <span class="pub-tag">CCF A</span>
            <span class="pub-tag">CORE A*</span>
            <span class="pub-tag">3D Vision</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/AInnovateLab/Context-aware-Indoor-PCG" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(calpcg_abs);">Abstract</a>
          |
          <a href="https://ainnovatelab.github.io/Context-aware-Indoor-PCG/" target="_blank">Project Page</a>
        </div>
      </div>
      <pre
        id="calpcg_abs">Indoor scene modification has emerged as a prominent area within computer vision, particularly for its applications in Augmented Reality (AR) and Virtual Reality (VR). Traditional methods often rely on pre-existing object databases and predetermined object positions, limiting their flexibility and adaptability to new scenarios. In response to this challenge, we present a novel end-to-end multi-modal deep neural network capable of generating point cloud objects seamlessly integrated with their surroundings, driven by textual instructions. Our work proposes a novel approach in scene modification by enabling the creation of new environments with previously unseen object layouts, eliminating the need for pre-stored CAD models. Leveraging Point-E as our generative model, we introduce innovative techniques such as quantized position prediction and Top-K estimation to address the issue of false negatives resulting from ambiguous language descriptions. Furthermore, we conduct comprehensive evaluations to showcase the diversity of generated objects, the efficacy of textual instructions, and the quantitative metrics, affirming the realism and versatility of our model in generating indoor objects. To provide a holistic assessment, we incorporate visual grounding as an additional metric, ensuring the quality and coherence of the scenes produced by our model. Through these advancements, our approach not only advances the state-of-the-art in indoor scene modification but also lays the foundation for future innovations in immersive computing and digital environment creation.</pre>
    </div>
  </div>
  <div class="pub-block">
    <img src="img/paper/wm_collision.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="https://arxiv.org/abs/2403.10020">
              Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs
            </a></div>
          <div class="pub-author">Yiyang Luo*, <b><u>Ke Lin*</u></b>, Chao Gu*, Ping Luo, Lijie Wen, Jiahui
            Hou</div>
          <div class="pub-conf">Under Review</div>
          <div>
            <span class="pub-tag">Watermark</span>
          </div>
        </div>
        <div class="pub-misc">
          <span>Code (TBD)</span>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(wm_collision_abs);">Abstract</a>
        </div>
      </div>
      <pre
        id="wm_collision_abs">The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.</pre>
    </div>
  </div>
  <div class="pub-block">
    <img src="img/paper/zgls.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="https://arxiv.org/abs/2403.10856">
              Zero-shot Generative Linguistic Steganography
            </a></div>
          <div class="pub-author"><b>Ke Lin</b>, Yiyang Luo, Zijian Zhang, Ping Luo</div>
          <div class="pub-conf">NAACL 2024</div>
          <div>
            <span class="pub-tag">CCF B</span>
            <span class="pub-tag">CORE A</span>
            <span class="pub-tag">Watermark</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/leonardodalinky/zero-shot-GLS" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(zgls_abs);">Abstract</a>
          |
          <a href="https://leonardodalinky.github.io/zero-shot-GLS/" target="_blank">Project Page</a>
        </div>
      </div>
      <pre
        id="zgls_abs">Generative linguistic steganography attempts to hide secret messages into covertext. Previous studies have generally focused on the statistical differences between the covertext and stegotext, however, ill-formed stegotext can readily be identified by humans. In this paper, we propose a novel zero-shot approach based on in-context learning for linguistic steganography to achieve better perceptual and statistical imperceptibility. We also design several new metrics and reproducible language evaluations to measure the imperceptibility of the stegotext. Our experimental results indicate that our method produces \(1.926\times\) more innocent and intelligible stegotext than any other method.</pre>
    </div>
  </div>
  <div class="pub-block">
    <img src="img/paper/ViRED.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="javascript:void(0);">
            ViRED: Prediction of Visual Relations in Engineering Drawings
            </a></div>
          <div class="pub-author">Chao Gu, <b>Ke Lin</b>, Yiyang Luo, Jiahui Hou, Xiang-Yang Li</div>
          <div class="pub-conf">Under Review</div>
          <div>
            <span class="pub-tag">2D Vision</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/AInnovateLab/ViRED" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(vired_abs);">Abstract</a>
          |
          <a href="https://ainnovatelab.github.io/ViRED/" target="_blank">Project Page</a>
        </div>
      </div>
      <pre
        id="vired_abs">To accurately understand engineering drawings, it is essential to establish the correspondence between images and their description tables within the drawings. Existing document understanding methods predominantly focus on text as the main modality, which is not suitable for documents containing substantial image information. In the field of visual relation detection, the structure of the task inherently limits its capacity to assess relationships among all entity pairs in the drawings. To address this issue, we propose a vision-based relation detection model, named ViRED, to identify the associations between tables and circuits in electrical engineering drawings. Our model mainly consists of three parts: a vision encoder, an object encoder, and a relation decoder. We implement ViRED using PyTorch to evaluate its performance. To validate the efficacy of ViRED, we conduct a series of experiments. The experimental results indicate that, within the engineering drawing dataset, our approach attained an accuracy of 96% in the task of relation prediction, marking a substantial improvement over existing methodologies. The results also show that ViRED can inference at a fast speed even when there are numerous objects in a single engineering drawing.</pre>
    </div>
  </div>
  <div class="pub-block">
    <img src="img/paper/llmpc.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="http://arxiv.org/abs/2407.18982">
              Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC
            </a></div>
          <div class="pub-author"><b>Ke Lin</b>, Yasir Glani, Ping Luo</div>
          <div class="pub-conf">IJCAI-2024 AISafety Workshop</div>
          <div>
            <span class="pub-tag">CCF A</span>
            <span class="pub-tag">CORE A*</span>
            <span class="pub-tag">Workshop</span>
            <span class="pub-tag">Security</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/leonardodalinky/CrypTen_LLMPC" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(llmpc_abs);">Abstract</a>
        </div>
      </div>
      <pre
        id="llmpc_abs">Secure multi-party computation (MPC) facilitates privacy-preserving computation between multiple parties without leaking private information.While most secure deep learning techniques utilize MPC operations to achieve feasible privacy-preserving machine learning on downstream tasks, the overhead of the computation and communication still hampers their practical application.This work proposes a low-latency secret-sharing-based MPC design that reduces unnecessary communication rounds during the execution of MPC protocols.We also present a method for improving the computation of commonly used nonlinear functions in deep learning by integrating multivariate multiplication and coalescing different packets into one to maximize network utilization.Our experimental results indicate that our method is effective in a variety of settings, with a speedup in communication latency of \(10\sim20\%\).</pre>
    </div>
  </div>
</div>

<hr />

<h2>Services</h2>

<ul>
  <li><b>Reviewer</b>: ACL, EMNLP, NAACL, MM, IJCAI</li>
  <li><b>Teaching Assistant</b>:
    <ul>
      <li>2021 Web Front-end Technology. Worked with <a
          href="https://www.thss.tsinghua.edu.cn/en/faculty/lijiewen.htm">Prof. Lijie Wen</a>.</li>
    </ul>
  </li>
</ul>
<!-- TA -->

<hr />

<h2>Awards and Scholarships</h2>

<ul>
  <li>2019 The Second Prize Scholarship</li>
</ul>