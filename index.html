---
layout: default
title: Ke Lin
---


<h2>About Me</h2>

<div class="aboutme">
  <div class="aboutme-content">
    <p>
      I am a second-year master student in Software Engineering, Tsinghua University, supervised by <a
        href="https://www.thss.tsinghua.edu.cn/en/faculty/pingluo.htm">Prof. Ping Luo</a>. I received my
      bachelor's
      degree in Software Engineering from Tsinghua University, advised by <a
        href="https://www.thss.tsinghua.edu.cn/en/faculty/lijiewen.htm">Prof. Lijie Wen</a>.
    </p>
    <p>Previously, I worked as a research intern at <a
        href="https://gcloud.tencent.com/pages/research/game-ai.html">Tencent
        Game AI Research Center</a>, with a mission to improve
      the realistic, reliability, and aesthetic of AI-generated 3D scenes.</p>
    <p>
      Currently, I am an independent researcher focusing on enhancing the understanding of 3D scenes through
      Natural Language Processing, a.k.a "NLP+3D".</p>
    <p style="color: rgb(205,97,85)">ðŸŒŸSeeking open positions for 2025 Fall Ph.D.ðŸŒŸ</p>
  </div>
  <div><img class="profile-picture" src="img/profile.jpg" /></div>
</div>

<hr />

<h2>Education</h2>

<div class="education">
  <div class="education-block">
    <img src="img/tsinghua-university-logo.png" />
    <div>
      <div class="education-school">Tsinghua University</div>
      <div class="education-date">2022.09 - 2025.06 (expected)</div>
      <div class="education-degree">Master of Engineering in Software Engineering</div>
      <div class="education-score">GPA: 3.52/4.0</div>
    </div>
  </div>
  <div class="education-block">
    <img src="img/tsinghua-university-logo.png" />
    <div>
      <div class="education-school">Tsinghua University</div>
      <div class="education-date">2018.09 - 2022.06</div>
      <div class="education-degree">Bachelor in Software Engineering</div>
      <div class="education-score">GPA: 3.68/4.0, Rank 25%</div>
    </div>
  </div>
</div>

<hr />

<h2>Research of Interest</h2>

<p>Previously, I focused on AI safety, including LLM Watermark, Linguistic Steganography and Privacy-preserving
  Computation.</p>

<b>My current Research of Interest are:</b>
<ul>
  <li>3D Scene Understanding & Arrangement</li>
  <li>Text-guided 3D Vision (NLP+3D)</li>
  <li>Multi-modal 3D LLMs</li>
  <li>Embodied AI</li>
</ul>

<hr />

<h2>Research Experiences</h2>

<div class="research">
  <div class="research-block">
    <img src="img/tencent-logo.png" />
    <div>
      <div class="research-institute"><a href="https://gcloud.tencent.com/pages/research/game-ai.html">Tencent -
          Game AI Research Center</a></div>
      <div class="research-date">2024.04 - 2024.09</div>
      <div class="research-role">Research Intern</div>
    </div>
  </div>
  <div class="research-block">
    <img src="img/momenta-logo.png" />
    <div>
      <div class="research-institute"><a href="https://www.momenta.cn/en/">Momenta</a></div>
      <div class="research-date">2021.01 - 2021.04</div>
      <div class="research-role">Research & Development Intern</div>
    </div>
  </div>
</div>

<hr />

<h2>Selected Publications</h2>

<div class="publication">
  <p>* denotes equal contribution.</p>
  <div class="pub-block">
    <img src="img/paper/wm_collision.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="https://arxiv.org/abs/2403.10020">
              Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs
            </a></div>
          <div class="pub-author">Yiyang Luo*, <b><u>Ke Lin*</u></b>, Chao Gu*, Jiahui Hou, Lijie Wen, Ping Luo</div>
          <div class="pub-conf">NAACL 2025 Findings</div>
          <div>
            <span class="pub-tag">CCF B</span>
            <span class="pub-tag">CORE A</span>
            <span class="pub-tag">Watermark</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/AInnovateLab/watermark-collision" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(wm_collision_abs);">Abstract</a>
          |
          <a href="https://ainnovatelab.github.io/watermark-collision/" target="_blank">Project Page</a>
        </div>
      </div>
      <pre
        id="wm_collision_abs">The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.</pre>
    </div>
  </div>
  <div class="pub-block">
    <img src="img/paper/calpcg.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="https://arxiv.org/abs/2311.16501">
              Context-Aware Indoor Point Cloud Object Generation through User Instructions
            </a></div>
          <div class="pub-author">Yiyang Luo*, <b>Ke Lin*</b>, Chao Gu</div>
          <div class="pub-conf">ACM MM 2024</div>
          <div>
            <span class="pub-tag">CCF A</span>
            <span class="pub-tag">CORE A*</span>
            <span class="pub-tag">3D Vision</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/AInnovateLab/Context-aware-Indoor-PCG" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(calpcg_abs);">Abstract</a>
          |
          <a href="https://ainnovatelab.github.io/Context-aware-Indoor-PCG/" target="_blank">Project Page</a>
        </div>
      </div>
      <pre
        id="calpcg_abs">Indoor scene modification has emerged as a prominent area within computer vision, particularly for its applications in Augmented Reality (AR) and Virtual Reality (VR). Traditional methods often rely on pre-existing object databases and predetermined object positions, limiting their flexibility and adaptability to new scenarios. In response to this challenge, we present a novel end-to-end multi-modal deep neural network capable of generating point cloud objects seamlessly integrated with their surroundings, driven by textual instructions. Our work proposes a novel approach in scene modification by enabling the creation of new environments with previously unseen object layouts, eliminating the need for pre-stored CAD models. Leveraging Point-E as our generative model, we introduce innovative techniques such as quantized position prediction and Top-K estimation to address the issue of false negatives resulting from ambiguous language descriptions. Furthermore, we conduct comprehensive evaluations to showcase the diversity of generated objects, the efficacy of textual instructions, and the quantitative metrics, affirming the realism and versatility of our model in generating indoor objects. To provide a holistic assessment, we incorporate visual grounding as an additional metric, ensuring the quality and coherence of the scenes produced by our model. Through these advancements, our approach not only advances the state-of-the-art in indoor scene modification but also lays the foundation for future innovations in immersive computing and digital environment creation.</pre>
    </div>
  </div>
  <div class="pub-block">
    <img src="img/paper/zgls.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="https://arxiv.org/abs/2403.10856">
              Zero-shot Generative Linguistic Steganography
            </a></div>
          <div class="pub-author"><b>Ke Lin</b>, Yiyang Luo, Zijian Zhang, Ping Luo</div>
          <div class="pub-conf">NAACL 2024</div>
          <div>
            <span class="pub-tag">CCF B</span>
            <span class="pub-tag">CORE A</span>
            <span class="pub-tag">Watermark</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/leonardodalinky/zero-shot-GLS" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(zgls_abs);">Abstract</a>
          |
          <a href="https://leonardodalinky.github.io/zero-shot-GLS/" target="_blank">Project Page</a>
        </div>
      </div>
      <pre
        id="zgls_abs">Generative linguistic steganography attempts to hide secret messages into covertext. Previous studies have generally focused on the statistical differences between the covertext and stegotext, however, ill-formed stegotext can readily be identified by humans. In this paper, we propose a novel zero-shot approach based on in-context learning for linguistic steganography to achieve better perceptual and statistical imperceptibility. We also design several new metrics and reproducible language evaluations to measure the imperceptibility of the stegotext. Our experimental results indicate that our method produces \(1.926\times\) more innocent and intelligible stegotext than any other method.</pre>
    </div>
  </div>
  <div class="pub-block">
    <img src="img/paper/llmpc.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="http://arxiv.org/abs/2407.18982">
              Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC
            </a></div>
          <div class="pub-author"><b>Ke Lin</b>, Yasir Glani, Ping Luo</div>
          <div class="pub-conf">IJCAI-2024 AISafety Workshop</div>
          <div>
            <span class="pub-tag">CCF A</span>
            <span class="pub-tag">CORE A*</span>
            <span class="pub-tag">Workshop</span>
            <span class="pub-tag">Security</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/leonardodalinky/CrypTen_LLMPC" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(llmpc_abs);">Abstract</a>
        </div>
      </div>
      <pre
        id="llmpc_abs">Secure multi-party computation (MPC) facilitates privacy-preserving computation between multiple parties without leaking private information.While most secure deep learning techniques utilize MPC operations to achieve feasible privacy-preserving machine learning on downstream tasks, the overhead of the computation and communication still hampers their practical application.This work proposes a low-latency secret-sharing-based MPC design that reduces unnecessary communication rounds during the execution of MPC protocols.We also present a method for improving the computation of commonly used nonlinear functions in deep learning by integrating multivariate multiplication and coalescing different packets into one to maximize network utilization.Our experimental results indicate that our method is effective in a variety of settings, with a speedup in communication latency of \(10\sim20\%\).</pre>
    </div>
  </div>
  <div class="pub-block">
    <img src="img/paper/ViRED.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="https://arxiv.org/abs/2409.00909">
              ViRED: Prediction of Visual Relations in Engineering Drawings
            </a></div>
          <div class="pub-author">Chao Gu, <b>Ke Lin</b>, Yiyang Luo, Jiahui Hou, Xiang-Yang Li</div>
          <div class="pub-conf">IEEE MSN 2024</div>
          <div>
            <span class="pub-tag">CCF C</span>
            <span class="pub-tag">2D Vision</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/AInnovateLab/ViRED" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(vired_abs);">Abstract</a>
          |
          <a href="https://ainnovatelab.github.io/ViRED/" target="_blank">Project Page</a>
        </div>
      </div>
      <pre
        id="vired_abs">To accurately understand engineering drawings, it is essential to establish the correspondence between images and their description tables within the drawings. Existing document understanding methods predominantly focus on text as the main modality, which is not suitable for documents containing substantial image information. In the field of visual relation detection, the structure of the task inherently limits its capacity to assess relationships among all entity pairs in the drawings. To address this issue, we propose a vision-based relation detection model, named ViRED, to identify the associations between tables and circuits in electrical engineering drawings. Our model mainly consists of three parts: a vision encoder, an object encoder, and a relation decoder. We implement ViRED using PyTorch to evaluate its performance. To validate the efficacy of ViRED, we conduct a series of experiments. The experimental results indicate that, within the engineering drawing dataset, our approach attained an accuracy of 96% in the task of relation prediction, marking a substantial improvement over existing methodologies. The results also show that ViRED can inference at a fast speed even when there are numerous objects in a single engineering drawing.</pre>
    </div>
  </div>
  <!-- Full Publications -->
  <div style="width: 100%; margin: 20px; text-align: center;">
    <a id="full-pub-list-button" href="javascript:void(0);" onclick="javascript:toggleFullPubList();">ðŸ”¼ Full
      Publications</a>
  </div>
  <div id="full-pub-list">
    <div class="pub-block">
      <img src="img/paper/mdasc.jpg" />
      <div class="pub-content-wrapper">
        <div class="pub-content">
          <div class="pub-innerblock">
            <!-- TODO -->
            <div class="pub-title"><a href="javascript:void(0);">
                MDASC: Advanced Dual-Layered Code Cloning Technique for Identifying Reused Malicious Code
              </a></div>
            <div class="pub-author">Yasir Glani, Ping Luo, <b>Ke Lin</b></div>
            <div class="pub-conf">SEKE 2024</div>
            <div>
              <span class="pub-tag">CCF C</span>
              <span class="pub-tag">CORE C</span>
              <span class="pub-tag">Security</span>
            </div>
          </div>
          <div class="pub-misc">
            <a href="javascript:void(0);" onclick="javascript:toggle(mdasc_abs);">Abstract</a>
          </div>
        </div>
        <pre
          id="mdasc_abs">Code cloning is a prevalent practice in software development, frequently exploited by adversaries to propagate malicious code, compromising user security and privacy. Recently proposed detection techniques often fail to identify complex clones, posing significant software integrity and security risks. In response, we introduce MDASC, an advanced dual-layered approach to detecting known Android malware through code cloning. MDASC employs method-based signature detection followed by sliding windows-based detection, effectively identifying modified and unmodified states of reused malware code, encompassing Type-I, Type-II, and Type-III (VST, ST, and MT) clones. Our comprehensive comparative study evaluates MDASC against recently proposed techniques across various code cloning detection methods, demonstrating its superiority in precision, recall, F1-score, accuracy, and efficiency. Leveraging intelligent retrieval and processing algorithms, MDASC maintains high efficiency, achieving an average detection time of 0.33 seconds for a database of 132,414 files and 13.5 million lines of code. This efficiency and effectiveness emphasize MDASCâ€™s potential for real-time security applications. MDASC represents a significant advancement in malicious code reuse detection, offering unparalleled accuracy and efficiency, thereby enhancing software security practices within the rapidly evolving Android ecosystem.</pre>
      </div>
    </div>
    <div class="pub-block">
      <img src="img/paper/ayatdroid.jpg" />
      <div class="pub-content-wrapper">
        <div class="pub-content">
          <div class="pub-innerblock">
            <div class="pub-title"><a href="https://ieeexplore.ieee.org/abstract/document/10217577">
                AyatDroid: A Lightweight Code Cloning Technique Using Different Static Features
              </a></div>
            <div class="pub-author">Yasir Glani, Ping Luo, <b>Ke Lin</b>, Syed Asad Shah</div>
            <div class="pub-conf">IEEE SEAI 2023</div>
            <div>
              <span class="pub-tag">Security</span>
            </div>
          </div>
          <div class="pub-misc">
            <a href="javascript:void(0);" onclick="javascript:toggle(ayatdroid_abs);">Abstract</a>
          </div>
        </div>
        <pre
          id="ayatdroid_abs">In recent decades, malicious code reuse has surged in numbers and sophistication, it is a common practice among adversaries to reuse malicious code, which significantly threatens user privacy and security. Several signature-based code clone detection techniques have been proposed to detect malicious clones in Android applications that use the MD5 hash function to generate signatures. Meanwhile, these techniques only retrieve signatures from Java files. Due to the 128-bit signature size of the MD5 hash function, these techniques take longer to generate signatures. In this article, we propose the AyatDroid technique, which efficiently identifies malicious chunks by retrieving signatures from Java and manifest files . AyatDroid technique is tested on reliable CiCMalDroid 2020 dataset. We have evaluated the AyatDroid technique with other cutting-edge code clone detection techniques. Our experimental results demonstrated that AyatDroid outperformed regarding detection time and accuracy. AyatDroid is not only lightweight but also efficient, allowing it to be implemented on the large scale.</pre>
      </div>
    </div>
    <div class="pub-block">
      <img src="img/paper/qcls.jpg" />
      <div class="pub-content-wrapper">
        <div class="pub-content">
          <div class="pub-innerblock">
            <div class="pub-title"><a href="https://journals.aps.org/pra/abstract/10.1103/PhysRevA.105.043707">
                Classical-to-quantum transition in multimode nonlinear systems with strong photon-photon coupling
              </a></div>
            <div class="pub-author">Yue-Xun Huang, Ming Li, <b>Ke Lin</b>, Yan-Lei Zhang, Guang-Can Guo, and Chang-Ling
              Zou</div>
            <div class="pub-conf">Physical Review A 2022</div>
            <div>
              <span class="pub-tag">JCR Q1</span>
              <span class="pub-tag">Quantum Computing</span>
            </div>
          </div>
          <div class="pub-misc">
            <a href="https://github.com/yesunhuang/QCLS" target="_blank">Code</a>
            |
            <a href="javascript:void(0);" onclick="javascript:toggle(qcls_abs);">Abstract</a>
          </div>
        </div>
        <pre
          id="qcls_abs">With advanced micro- and nanophotonic structures, the vacuum photon-photon coupling rate is anticipated to approach the intrinsic loss rate and lead to unconventional quantum effects. Here, we investigate the classical-to-quantum transition of such photonic nonlinear systems using the quantum cluster-expansion method, which addresses the computational challenge in tracking large photon number states of the fundamental and harmonic optical fields involved in the second-harmonic generation process. Compared to the mean-field approximation used in the weak-coupling limit, the quantum cluster-expansion method solves multimode dynamics efficiently and reveals the quantum behaviors of optical parametric oscillations around the threshold. This paper presents a universal tool to study quantum dynamics of multimode systems and explore the nonlinear photonic devices for continuous-variable quantum information processing.</pre>
      </div>
    </div>
  </div>
</div>

<hr />

<h2>Services</h2>

<ul>
  <li><b>Reviewer</b>: ACL, EMNLP, NAACL, MM, IJCAI</li>
  <li><b>Teaching Assistant</b>:
    <ul>
      <li>2021 Web Front-end Technology. Worked with <a
          href="https://www.thss.tsinghua.edu.cn/en/faculty/lijiewen.htm">Prof. Lijie Wen</a>.</li>
    </ul>
  </li>
</ul>
<!-- TA -->

<hr />

<h2>Awards and Scholarships</h2>

<ul>
  <li>2024 Tsinghua-Ubiquant Scholarship</li>
  <li>2019 The Second Prize Scholarship</li>
</ul>